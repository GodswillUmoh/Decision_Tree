# Decision_Tree

## Explanation of Decison Tree
> A decision tree is a machine learning algorithm used for classification and regression tasks. It works like a flowchart, where each node represents a decision based on a feature, and the branches represent the possible outcomes of that decision. The tree splits data into subsets, eventually leading to a prediction or classification at the leaf nodes.

## The CART Concept
It means Classification and Regression Trees
> CART, short for Classification and Regression Trees, is a machine learning algorithm used to build decision trees for classification or regression tasks. It creates binary decision trees, meaning each decision splits the data into two subsets.

## Explanation of CART
> + Classification Trees: Used when the target variable is categorical (e.g., predicting "Yes" or "No").
> + Regression Trees: Used when the target variable is continuous (e.g., predicting a numerical value like house prices).

## Advantages of CART:
> + Handles Non-Linear Data: Works well with non-linear relationships.
> + Easy Interpretation: Trees are simple to visualize and explain.
> + No Need for Feature Scaling: Works with raw data without normalization or standardization.

## Disadvantages of CART:
> + Prone to Overfitting: Can create overly complex trees that capture noise in the data.
Sensitive to Data: Small changes in the dataset can lead to a completely different tree.
> + Less Accurate for Large Datasets: May underperform compared to ensemble methods like Random Forest or Gradient Boosting.

## Decision Tree Graph
+ [Click here to view graph explanation](https://ibb.co/PNzWB2P)
+ [View Another Graph, Click Here](https://ibb.co/db8W8ZC)
